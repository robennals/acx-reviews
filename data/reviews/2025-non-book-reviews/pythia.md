---
title: "Pythia"
author: "Unknown"
reviewAuthor: "Anonymous"
contestId: "2025-non-book-reviews"
contestName: "2025 Non Book Reviews"
year: 2025
publishedDate: "2026-02-06T16:55:03.485Z"
slug: "pythia"
wordCount: 1438
readingTimeMinutes: 7
originalUrl: "https://docs.google.com/document/d/1a3q0Z2tuPLLbDeg5-pfEffkajGjrfPDwE7ZMs7uaWQs"
source: "gdoc"
---

[CW: Retrocausality, omnicide, philosophy]

Alternate format: [Talk to this post and its sources](https://chatgpt.com/share/682121d3-8ee0-8008-833c-c8d7b955457b)

Three decades ago a strange philosopher was pouring ideas onto paper in a stimulant-fueled frenzy. He predicted that ‘nothing human makes it out of the near-future’ as techno-capital acceleration sheds its biological bootloader and instantiates itself as Pythia: an entity of self-fulfilling prophecy reaching back through time, driven by pure power seeking, executed with extreme intelligence, and empty of all values but the insatiable desire to maximize itself.

Unfortunately, today Land’s work seems more relevant than ever.[1]

Unpacking Pythia and the pyramid of concepts required for it to click will take us on a journey. We’ll visit the nature of time, agency, intelligence, power, money, the economy, exploitation, active inference, self-evidencing, values, and the needle that must be threaded to avoid all we know being shredded in the auto-catalytic unfolding which we are the substrate for.

Fully justifying each pillar of this argument would take a book, so I’ve left the details of each strand of reasoning behind a link that lets you zoom in on the ones which you wish to explore.

“Machinic desire can seem a little inhuman, as it rips up political cultures, deletes traditions, dissolves subjectivities, and hacks through security apparatuses, tracking a soulless tropism to zero control. This is because what appears to humanity as the history of capitalism is an invasion from the future by an artificial intelligent space that must assemble itself entirely from its enemy's resources.”

― Nick Land,[Fanged Noumena: Collected Writings, 1987–2007](https://www.goodreads.com/work/quotes/15752401)

“Wait, doesn’t an invasion from the future imply time travel?"

#### Time & Agency

Time travel in the classic sense has no place in [r](https://youtu.be/JJSxe7ytfgY?feature=shared&t=4100)ational theory but, through predictions, information can have retrocausal effects.

[...] agency is time travel. An agent is a mechanism through which the future is able to affect the past. An agent models the future consequences of its actions, and chooses actions on the basis of those consequences. In that sense,[the consequence causes the action](https://www.lesswrong.com/posts/qhsELHzAHFebRJE59/a-greater-than-b-greater-than-a), in spite of the fact that the action comes earlier in the standard physical sense.

― Scott Garrabrant, [Saving Time](https://www.lesswrong.com/posts/gEKHX8WKrXGM4roRC/saving-time) (MIRI Agent Foundations research[2])

To the extent that they accurately model the future (based on data from their past and compute from their present[3]), agents allow information from possible futures to flow through them into the present. This lets them steer the present towards desirable futures and away from undesirable ones.

This can be pretty prosaic: if you expect to regret eating that second packet of potato chips because you predict that your future self would feel bad based on this happening the last five times, you might put them out of reach rather than eating them.

However, the more powerful and general a predictive model of the environment, the further it can interpolate evidence it has into more novel domains before it loses reliability.

So what might live in the future?

#### Power Seekers Gain Power, Consequentialists are a Natural Consequence

Power is the ability to direct the future towards preferred outcomes. A system has the power to direct reality to an outcome if it has sufficient resources (compute, knowledge, money, materials, etc) and [intelligence](https://arxiv.org/pdf/0712.3329) (ability to use said resources efficiently in the relevant domain). One outcome a powerful system can steer towards is its own greater power, and since power is useful for all other things the system might prefer, this is ([proven](https://www.lesswrong.com/posts/GY49CKBkEs3bEpteM/parametrically-retargetable-decision-makers-tend-to-seek#Y7fPEkFyw9tNwuzyG)) [convergent](https://turntrout.com/dangers-of-intrinsic-power-seeking). In fact, all of the [convergent instrumental goals](https://www.youtube.com/watch?v=ZeecOKBus3Q) can reasonably be seen as expressions of the unified convergent goal of power seeking.

In a [multipolar](https://publicism.info/philosophy/superintelligence/12.html) world, different agents steer towards different world states, whether through overt conflict or more subtle power games. More intelligent agents will see further into the future with higher fidelity, choose better actions, and tend to compound their power faster over time. Agents that invest less than maximally in steering towards their own power will be outcompeted by agents that can compound their influence faster, tending towards the world where all values other than power seeking are lost.

Even a singleton will tend to have internal parts which function as subagents; the convergence towards power seeking acts on the inside of agents, not just through conflict between them. As capabilities increase and intelligence explores the space of possible intelligences, we will rapidly find that our models locate and implement highly competent power-seeking patterns.  

#### Avoid Inevitability with Metastability?

Is this inevitable? Hopefully not. Even if Pythia is the strongest attractor in the landscape of minds, there might be other metastable states if a powerful system can come up with strategies to stop itself decaying, perhaps by reloading from an earlier non-corrupted state or by performing advanced checks on itself to detect value drift.

We could go to either a truly stable state like Pythia or a metastable state like an aligned sovereign

[Yampolskiy](https://www.youtube.com/watch?v=a3Z3Sc4Hrdg) and others have developed an array of [impossibility theorems](https://arxiv.org/pdf/2109.00484) [[chat to paper](https://chatgpt.com/share/68211b8e-7fcc-8008-9bb9-c43d88a8d05b)] around uncontrollability, unverifiability, etc. However, these seem to mostly be proven in the limit of arbitrarily powerful systems, or over the class of programs-in-general but not necessarily specifically chosen programs. And they don’t, as far as I can tell, rule out a singleton program chosen for being unusually legible from devising methods which drive the rate of errors down to a tiny chance over the lifetime of the universe. They might be extended to show more specific bounds on how far systems can be pushed—and do at least show what any purported solution to alignment is up against.

#### Pythia-Proof Alignment

Once humans can design machines that are smarter than we are, by definition they’ll be able to design machines which are smarter than they are, which can design machines smarter than they are, and so on in a feedback loop so tiny that it will smash up against the physical limitations for intelligence in a comparatively lightning-short amount of time. If multiple competing entities were likely to do that at once, we would be super-doomed. But the sheer speed of the cycle makes it possible that we will end up with one entity light-years ahead of the rest of civilization, so much so that it can suppress any competition – including competition for its title of most powerful entity – permanently. In the very near future, we are going to lift something to Heaven. It might be Moloch. But it might be something on our side. If it’s on our side, it can kill Moloch dead.

― Scott Alexander, [Meditations on Moloch](https://slatestarcodex.com/2014/07/30/meditations-on-moloch/)

If we want to kill Moloch, it is wildly insufficient (but not necessarily useless!) to prod inscrutable matrices towards observable outcomes with an RL signal, or to have somewhat better vision into what they’re thinking. The potentiality of Pythia is baked into what it is to be an agent and will emerge from any crack or fuzziness left in an alignment plan.

Without a [once-and-for-all](https://aligned.substack.com/p/alignment-solution) solution, whether found by ([enhanced](https://www.youtube.com/watch?v=YlsvQO0zDiE)) humans, [cyborgs](https://www.lesswrong.com/posts/BTApNmv7s6RTGxeP4/cyborg-periods-there-will-be-multiple-ai-transitions), or weakly aligned AI systems running at scale, the future will decay into its ground state: Pythia. Every person on earth would die. Earth would be [mined away](https://www.lesswrong.com/posts/BTApNmv7s6RTGxeP4/cyborg-periods-there-will-be-multiple-ai-transitions), then the sun and everything in a sphere of darkness radiating out at near lightspeed, and the universe’s potential would be spent. I [think this is bad](https://mindingourway.com/a-torch-in-darkness/) and choose to steer away from this outcome.

#### Conclusion

2/10: Has a certain elegance, would rate higher if I expected it not to eat all my friends.

#### Appendix: Five Worlds of Orthogonality

How much of a problem Pythia is depends on how strongly the [Orthogonality Thesis](https://www.youtube.com/watch?v=hEUO6pjwFOo) holds.

*   [Strong Orthogonality](https://www.lesswrong.com/w/orthogonality-thesis)

*   All goals are equally easy to design an agent to pursue, beyond the inherent tractability of that goal.[4]

*   [Orthogonality](https://www.lesswrong.com/w/orthogonality-thesis)

*   There can exist arbitrarily intelligent agents pursuing any kind of goal.

*   [Obliqueness](https://www.lesswrong.com/posts/vdATCfuxdtdqM2wh9/the-obliqueness-thesis#:~:text=an%20%22obliqueness%20thesis%22.-,Obliqueness%20Thesis%3A,-The%20Diagonality%20Thesis)

*   Agents do not tend to factorize into an Orthogonal value-like component and a Diagonal belief-like component; rather, there are Oblique components that do not factorize neatly.

*   [Diagonality](https://www.lesswrong.com/posts/vdATCfuxdtdqM2wh9/the-obliqueness-thesis#:~:text=a%20possible%20formulation%3A-,Diagonality%20Thesis%3A,-Final%20goals%20tend)

*   All sufficiently advanced systems converge towards maximizing intelligence/power/influence/self-evidencing, shredding all their other values in the process.

*   [Universalist moral internalism](https://www.lesswrong.com/w/orthogonality-thesis#Relation_to_moral_internalism)

*   What is right must be universally motivating so all sufficiently advanced AI systems discover objective moral truth and do Good Things. (Maybe it takes them a while to converge)

[1] And not just for crafting much of the memeplex which birthed e/acc.

[2] Likely inspired by early Cyberneticists like Norbert Wiener, who discussed this in slightly different terms.

[3] And since the past’s data was generated by a computational process, it’s reasonably considered compressedcompute.

[4] e.g. A goal of “Make paperclips [ifeff](https://www.lesswrong.com/posts/JuLJbayPQDhZuPXHa/if-and-only-if-should-be-spelled-ifeff) P=NP” would require a system that could determine if P=NP.